copied over from the end of the turing machine article, since it probably deserves its own post.

## Edwin Hutchins on the Chinese Room

Hutchins continues immediately from the last passage. You will notice the similarity to modern day
commentaries on LLMs as "stochastic parrots."

> John Searle's "Chinese room" thought experiment provides a good example of this effect. Imagine a
> room inside of which sits the philosopher Searle. Chinese people come up to the room and push
> strings of Chinese characters through a slot in the door. Searle slips back other strings of
> characters, which the Chinese take to be clever responses to their questions. Now, Searle does not
> understand Chinese. He doesn't know the meaning of any Chinese character. To him, the characters
> of written Chinese are just a bunch of elaborate squiggles. However, Searle has with him in the
> room baskets of Chinese characters, and he has a rulebook which says that if he gets certain
> sequences of characters he should create certain other sequences of characters and slide them out
> the slot.
>
> Searle intends his thought experiment as a demonstration that syntax is not sufficient to produce
> semantics. According to Searle, the room appears to behave as though it understand Chinese; yet
> neither he nor anything in the room can be said to understand Chinese. There are many arguments
> for and against Searle's claims, and I will not review them here. Instead, I want to interpret the
> Chinese room in a completely different way: The Chinese room is a sociocultural cognitive system.
> The really nice thing about it is that it shows us very clearly that the cognitive properties of
> the person in the room are not the same as the cognitive properties of the room as a whole. There
> is John Searle with a basket of Chinese characters and a rulebook. Together he and the characters
> and rulebook in interaction seem to speak Chinese. But Searle himself speaks not a word of Chinese.

("Cognition in the Wild" - Edwin Hutchins pp360-362)

The idea that a system can have more cognitive behaviors than the sum of its parts is much older
than the current LLM debate.